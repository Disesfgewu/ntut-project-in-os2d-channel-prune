from os2d.engine.objective import Os2dObjective
import torch
import torch.nn as nn
import numpy as np
import gc
from contextlib import contextmanager

@contextmanager
def memory_cleanup():
    """è¨˜æ†¶é«”æ¸…ç†ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    try:
        yield
    finally:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        gc.collect()

import csv
import os
from datetime import datetime

class LossLogger:
    def __init__(self, csv_path="loss_log.csv"):
        self.csv_path = csv_path
        # å¦‚æœæª”æ¡ˆä¸å­˜åœ¨ï¼Œå¯«å…¥æ¨™é¡Œ
        if not os.path.exists(csv_path):
            with open(csv_path, mode='w', newline='') as file:
                writer = csv.writer(file)
                writer.writerow([
                    "timestamp", "batch_idx", "original_loss", "auxiliary_loss", 
                    "normalized_auxiliary_loss", "total_loss", "normalization_factor",
                    "auxiliary_weight", "ema_original", "ema_auxiliary"
                ])
    
    def log_loss(self, batch_idx, original_loss, auxiliary_loss, normalized_auxiliary_loss, 
                 total_loss, normalization_factor, auxiliary_weight, ema_original, ema_auxiliary):
        """
        è¨˜éŒ„æå¤±æ•¸æ“šåˆ° CSV
        åªæœ‰ç•¶æ‰€æœ‰æå¤±éƒ½ > 0 æ™‚æ‰è¨˜éŒ„
        """
        # æª¢æŸ¥æ˜¯å¦æœ‰ä»»ä½•æå¤±ç‚º 0 æˆ–ç„¡æ•ˆ
        if (original_loss <= 0 or auxiliary_loss <= 0 or 
            normalized_auxiliary_loss <= 0 or total_loss <= 0):
            return False
        
        try:
            with open(self.csv_path, mode='a', newline='') as file:
                writer = csv.writer(file)
                writer.writerow([
                    datetime.now().isoformat(),
                    batch_idx,
                    float(original_loss),
                    float(auxiliary_loss),
                    float(normalized_auxiliary_loss),
                    float(total_loss),
                    float(normalization_factor),
                    float(auxiliary_weight),
                    float(ema_original),
                    float(ema_auxiliary)
                ])
            return True
        except Exception as e:
            print(f"[LCP Logger] Error writing to CSV: {e}")
            return False
        
class LCPFinetuneCriterion(nn.Module):
    def __init__(self, original_criterion, aux_net, auxiliary_weight=1.0):
        """
        ä½¿ç”¨çµ„åˆæ¨¡å¼åŒ…è£ Os2dObjective
        
        Args:
            original_criterion: Os2dObjective å¯¦ä¾‹
            aux_net: è¼”åŠ©ç¶²è·¯å¯¦ä¾‹
            auxiliary_weight: è¼”åŠ©æå¤±æ¬Šé‡
        """
        super().__init__()
        self.original_criterion = original_criterion
        self.aux_net = aux_net
        self.auxiliary_weight = auxiliary_weight
        self.loss_logger = LossLogger()
    
    def forward(self, loc_preds, loc_targets, cls_preds, cls_targets,
                cls_targets_remapped=None, cls_preds_for_neg=None,
                patch_mining_mode=False, batch_idx=None):
        """å§”è¨—çµ¦åŸå§‹ criterion ä¸¦åŠ å…¥è¼”åŠ©æå¤± - æå¤±æ­£è¦åŒ–ç‰ˆæœ¬"""
        
        # åˆå§‹åŒ–æŒ‡æ•¸ç§»å‹•å¹³å‡å€¼ (ç”¨æ–¼å‹•æ…‹æ­£è¦åŒ–)
        if not hasattr(self, 'loss_ema'):
            self.loss_ema = {
                'original': 1.0,
                'auxiliary': 1.0,
                'decay': 0.99
            }
        
        # åˆå§‹åŒ–çµ±è¨ˆè¨ˆæ•¸å™¨
        if not hasattr(self, 'loss_stats'):
            self.loss_stats = {
                'total_batches': 0,
                'valid_combinations': 0,
                'invalid_original': 0,
                'invalid_auxiliary': 0
            }
        
        # å§”è¨—çµ¦åŸå§‹ Os2dObjective
        original_result = self.original_criterion(
            loc_preds, loc_targets, cls_preds, cls_targets,
            cls_targets_remapped, cls_preds_for_neg, patch_mining_mode
        )
        
        # è™•ç†è¿”å›å€¼
        if patch_mining_mode:
            original_losses, losses_per_anchor = original_result
        else:
            original_losses = original_result
        
        # è¨ˆç®—ä¸¦åŠ å…¥è¼”åŠ©æå¤± - ä½¿ç”¨ batch_idx è€Œä¸æ˜¯ batch_data
        self.get_dataloader().shuffle()
        auxiliary_loss = self.compute_auxiliary_loss_from_batch(batch_idx)
        original_loss = original_losses["loss"]
        
        # æ›´æ–°çµ±è¨ˆè¨ˆæ•¸å™¨
        self.loss_stats['total_batches'] += 1
        
        # ğŸ”´ æœ‰æ•ˆæ€§æª¢æŸ¥ï¼šåªæœ‰ç•¶å…©å€‹æå¤±éƒ½æœ‰æ•ˆæ™‚æ‰è¨ˆç®—
        VALIDITY_THRESHOLD = 1e-6
        original_is_valid = original_loss.item() > VALIDITY_THRESHOLD
        auxiliary_is_valid = auxiliary_loss.item() > VALIDITY_THRESHOLD
        
        print(f"[LCP Loss] Original loss: {original_loss:.6f} (valid: {original_is_valid})")
        print(f"[LCP Loss] Auxiliary loss: {auxiliary_loss:.6f} (valid: {auxiliary_is_valid})")
        
        # åªæœ‰ç•¶å…©å€‹æå¤±éƒ½æœ‰æ•ˆæ™‚æ‰é€²è¡Œçµ„åˆ
        if original_is_valid and auxiliary_is_valid:
            print("[LCP Info] âœ… Both losses are valid, applying normalization")
            
            # æ›´æ–°çµ±è¨ˆ
            self.loss_stats['valid_combinations'] += 1
            
            # ğŸ”´ æ›´æ–°æŒ‡æ•¸ç§»å‹•å¹³å‡å€¼
            self.loss_ema['original'] = (self.loss_ema['decay'] * self.loss_ema['original'] + 
                                        (1 - self.loss_ema['decay']) * original_loss.item())
            self.loss_ema['auxiliary'] = (self.loss_ema['decay'] * self.loss_ema['auxiliary'] + 
                                        (1 - self.loss_ema['decay']) * auxiliary_loss.item())
            
            # ğŸ”´ è¨ˆç®—å‹•æ…‹æ­£è¦åŒ–ä¿‚æ•¸
            # ç›®æ¨™ï¼šä½¿è¼”åŠ©æå¤±èˆ‡åŸå§‹æå¤±åœ¨ç›¸åŒé‡ç´š
            normalization_factor = self.loss_ema['original'] / (self.loss_ema['auxiliary'] + 1e-8)
            
            # é™åˆ¶æ­£è¦åŒ–ä¿‚æ•¸åœ¨åˆç†ç¯„åœå…§
            normalization_factor = max(0.05, min(2.0, normalization_factor))
            
            # ğŸ”´ æ‡‰ç”¨æ­£è¦åŒ–
            normalized_auxiliary = auxiliary_loss * normalization_factor
            
            # æ ¹æ“š LCP è«–æ–‡å…¬å¼è¨ˆç®—ç¸½æå¤±
            total_loss = original_loss + self.auxiliary_weight * normalized_auxiliary
            
            print(f"[LCP Normalization] EMA Original: {self.loss_ema['original']:.6f}")
            print(f"[LCP Normalization] EMA Auxiliary: {self.loss_ema['auxiliary']:.6f}")
            print(f"[LCP Normalization] Normalization factor: {normalization_factor:.6f}")
            print(f"[LCP Normalization] Normalized auxiliary: {normalized_auxiliary:.6f}")
            print(f"[LCP Loss] Auxiliary weight: {self.auxiliary_weight}")
            print(f"[LCP Loss] Total loss: {total_loss:.6f}")
            
            # ğŸ”´ è¨˜éŒ„æœ‰æ•ˆçš„æå¤±æ•¸æ“šåˆ° CSV
            log_success = self.loss_logger.log_loss(
                batch_idx=batch_idx if batch_idx is not None else self.loss_stats['total_batches'],
                original_loss=original_loss.item(),
                auxiliary_loss=auxiliary_loss.item(),
                normalized_auxiliary_loss=normalized_auxiliary.item(),
                total_loss=total_loss.item(),
                normalization_factor=normalization_factor,
                auxiliary_weight=self.auxiliary_weight,
                ema_original=self.loss_ema['original'],
                ema_auxiliary=self.loss_ema['auxiliary']
            )


            # ğŸ”´ é—œéµä¿®æ”¹ï¼šåªåŒ…å« Tensor é¡å‹çš„æå¤±å€¼
            lcp_losses = original_losses.copy()
            lcp_losses["loss"] = total_loss
            lcp_losses["auxiliary_loss"] = auxiliary_loss
            lcp_losses["normalized_auxiliary_loss"] = normalized_auxiliary
            lcp_losses["original_loss"] = original_loss
            
            # ğŸ”´ å°‡æ­£è¦åŒ–ä¿‚æ•¸è½‰æ›ç‚º Tensor ä»¥é¿å…é¡å‹éŒ¯èª¤
            lcp_losses["normalization_factor"] = torch.tensor(normalization_factor, 
                                                            device=original_loss.device, 
                                                            dtype=original_loss.dtype)
            
            # ğŸ”´ ç§»é™¤å¸ƒæ—å€¼ï¼Œæ”¹ç‚ºå­˜å„²åœ¨é¡å±¬æ€§ä¸­
            self.last_loss_valid = True
            
        else:
            # ä»»ä½•ä¸€å€‹æå¤±ç„¡æ•ˆæ™‚ï¼Œè·³éçµ„åˆ
            if not original_is_valid:
                print("[LCP Warning] âŒ Original loss is invalid (â‰¤ 1e-6)")
                self.loss_stats['invalid_original'] += 1
            if not auxiliary_is_valid:
                print("[LCP Warning] âŒ Auxiliary loss is invalid (â‰¤ 1e-6)")
                self.loss_stats['invalid_auxiliary'] += 1
            
            print("[LCP Info] ğŸš« Loss combination skipped - invalid losses detected")
            
            # è¿”å›åŸå§‹æå¤±ï¼Œæ¨™è¨˜ç‚ºç„¡æ•ˆ
            lcp_losses = original_losses.copy()
            lcp_losses["auxiliary_loss"] = auxiliary_loss
            lcp_losses["original_loss"] = original_loss
            
            # ğŸ”´ ç§»é™¤å¸ƒæ—å€¼ï¼Œæ”¹ç‚ºå­˜å„²åœ¨é¡å±¬æ€§ä¸­
            self.last_loss_valid = False
            
            # ğŸ”´ é—œéµä¿®æ”¹ï¼šä½¿ç”¨åŸå§‹æå¤±è€Œä¸æ˜¯æ¥µå°å€¼ï¼Œé¿å…è¨“ç·´åœæ»¯
            # ç•¶è¼”åŠ©æå¤±ç„¡æ•ˆæ™‚ï¼Œä»ç„¶å¯ä»¥ç”¨åŸå§‹æå¤±é€²è¡Œè¨“ç·´
            lcp_losses["loss"] = original_loss
            
            # æ·»åŠ é›¶å€¼çš„æ­£è¦åŒ–ä¿‚æ•¸ä½œç‚º Tensor
            lcp_losses["normalization_factor"] = torch.tensor(0.0, 
                                                            device=original_loss.device, 
                                                            dtype=original_loss.dtype)
        
        # ğŸ”´ å®šæœŸè¼¸å‡ºçµ±è¨ˆä¿¡æ¯
        if self.loss_stats['total_batches'] % 100 == 0:
            self.print_loss_statistics()
        
        if patch_mining_mode:
            return lcp_losses, losses_per_anchor
        else:
            return lcp_losses

    def print_loss_statistics(self):
        """åˆ—å°æå¤±çµ±è¨ˆä¿¡æ¯"""
        stats = self.loss_stats
        total = stats['total_batches']
        
        if total > 0:
            print(f"\n[LCP Statistics] After {total} batches:")
            print(f"  Valid combinations: {stats['valid_combinations']} ({stats['valid_combinations']/total*100:.1f}%)")
            print(f"  Invalid original: {stats['invalid_original']} ({stats['invalid_original']/total*100:.1f}%)")
            print(f"  Invalid auxiliary: {stats['invalid_auxiliary']} ({stats['invalid_auxiliary']/total*100:.1f}%)")
            print(f"  Overall validity rate: {stats['valid_combinations']/total*100:.1f}%")
            
            # å¦‚æœæœ‰æ•ˆç‡éä½ï¼Œæä¾›å»ºè­°
            if stats['valid_combinations']/total < 0.3:
                print(f"  âš ï¸  WARNING: Low validity rate, consider checking data pipeline")
            print()

    def get_loss_status(self):
        """ç²å–ç•¶å‰æå¤±ç‹€æ…‹"""
        return {
            'is_valid': getattr(self, 'last_loss_valid', False),
            'ema_original': self.loss_ema['original'] if hasattr(self, 'loss_ema') else 0.0,
            'ema_auxiliary': self.loss_ema['auxiliary'] if hasattr(self, 'loss_ema') else 0.0,
            'statistics': self.loss_stats if hasattr(self, 'loss_stats') else {}
        }

    def get_image_ids_for_batch_index(self, batch_idx):
        """ç²å–æŒ‡å®šæ‰¹æ¬¡ç´¢å¼•çš„åœ–åƒ ID"""
        return self.aux_net.get_contextual_roi_align().dataloader.get_image_ids_for_batch_index(batch_idx)

    def get_dataloader(self):
        """ç²å–æ•¸æ“šåŠ è¼‰å™¨"""
        return self.aux_net.get_contextual_roi_align().dataloader

    def get_database(self):
        return self.aux_net.get_db()

    def compute_auxiliary_loss_from_batch(self, batch_idx):
      """å¾æ‰¹æ¬¡æ•¸æ“šè¨ˆç®—è¼”åŠ©æå¤± - æ¥µè‡´ GPU RAM å„ªåŒ–ç‰ˆæœ¬"""
      
      # è¨˜æ†¶é«”ç›£æ§å‡½æ•¸
      def log_memory(stage):
          if torch.cuda.is_available():
              allocated = torch.cuda.memory_allocated(0) / 1024**3
              reserved = torch.cuda.memory_reserved(0) / 1024**3
              print(f"[Memory] {stage}: å·²åˆ†é… {allocated:.2f} GB, å·²ä¿ç•™ {reserved:.2f} GB")
      
      # å¼·åˆ¶è¨˜æ†¶é«”æ¸…ç†
      def force_cleanup():
          if torch.cuda.is_available():
              torch.cuda.empty_cache()
              torch.cuda.synchronize()
          gc.collect()
      
      print(f"[LCP Debug] Computing auxiliary loss...")
      print(f"[LCP Debug] batch_idx is None: {batch_idx is None}")
      print(f"[LCP Debug] aux_net is None: {self.aux_net is None}")
      
      log_memory("å‡½æ•¸é–‹å§‹")

      if batch_idx is None or self.aux_net is None:
          print(f"[LCP Debug] Returning zero auxiliary loss")
          return torch.tensor(0.0, requires_grad=True)
      
      device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
      
      try:
          # ğŸ”´ éšæ®µ1: ç²å–åŸºæœ¬è³‡æ–™ä¸¦ç«‹å³æ¸…ç†
          with memory_cleanup():
              image_ids = self.get_image_ids_for_batch_index(batch_idx)
              log_memory("ç²å– image_ids å¾Œ")
          
          # ğŸ”´ éšæ®µ2: åˆ†æ‰¹æ¬¡è™•ç†è³‡æ–™
          batch = self.get_dataloader().get_batch(batch_idx)
          
          # ç«‹å³è§£åŒ…ä¸¦æ¸…ç†åŸå§‹ batch
          images, class_images, loc_targets, cls_targets, class_ids, class_sizes, transforms, boxes, img_sizes = batch
          del batch
          force_cleanup()
          log_memory("æ‰¹æ¬¡è³‡æ–™è§£åŒ…å¾Œ")

          # èª¿è©¦è³‡è¨Š
          print(f"[LCP Debug] class_ids type: {type(class_ids)}")
          print(f"[LCP Debug] class_ids length: {len(class_ids) if hasattr(class_ids, '__len__') else 'No length'}")
          if len(class_ids) > 0:
              print(f"[LCP Debug] class_ids[0] type: {type(class_ids[0])}")

          # ğŸ”´ éšæ®µ3: æ¥µè‡´æ¨£æœ¬æ•¸é‡é™åˆ¶
          db = self.get_database()
          
          # æ›´åš´æ ¼çš„é™åˆ¶ï¼šæœ€å¤š 8 å€‹æ¨£æœ¬
          MAX_SAMPLES = min( 8 , len(image_ids) )
          samples_collected = 0
          total_aux_loss = torch.tensor(0.0, device=device, requires_grad=True)
          
          # ğŸ”´ ç›´æ¥è™•ç†æ¨£æœ¬ï¼Œä¸å…ˆæ”¶é›†
          for i, image_id in enumerate(image_ids):
              if samples_collected >= MAX_SAMPLES:
                  break
              
              # æ¯è™•ç†5å€‹ image_id å°±æ¸…ç†ä¸€æ¬¡è¨˜æ†¶é«”
              if i % 5 == 0:
                  force_cleanup()
                  log_memory(f"è™•ç† image_id {i}")
              
              # è™•ç† class_ids çš„ä¸åŒçµæ§‹
              if isinstance(class_ids[i], (list, tuple, np.ndarray)):
                  current_class_ids = class_ids[i]
              else:
                  current_class_ids = [class_ids[i]]
              
              for j, class_id in enumerate(current_class_ids):
                  if samples_collected >= MAX_SAMPLES:
                      break
                      
                  class_id = int(class_id)
                  
                  try:
                      # ğŸ”´ é€å€‹ç²å–ä¸¦ç«‹å³ä½¿ç”¨è³‡æ–™
                      point1_xs = db.get_specific_data(image_id, class_id, 'point1_x')
                      point1_ys = db.get_specific_data(image_id, class_id, 'point1_y')
                      point2_xs = db.get_specific_data(image_id, class_id, 'point2_x')
                      point2_ys = db.get_specific_data(image_id, class_id, 'point2_y')

                      # æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§
                      if not (point1_xs and point1_ys and point2_xs and point2_ys):
                          continue
                      
                      # åªè™•ç†ç¬¬ä¸€å€‹æœ‰æ•ˆæ¨£æœ¬
                      min_length = min(len(point1_xs), len(point1_ys), len(point2_xs), len(point2_ys))
                      
                      if min_length > 0:
                          try:
                              # åªå–ç¬¬ä¸€å€‹æ¨£æœ¬
                              point1 = (float(point1_xs[0]), float(point1_ys[0]))
                              point2 = (float(point2_xs[0]), float(point2_ys[0]))
                              
                              # ğŸ”´ ç«‹å³è¨ˆç®—è¼”åŠ©æå¤±
                              with memory_cleanup():
                                  sample_aux_loss = self.aux_net.aux_loss(
                                      image_id, 
                                      class_id, 
                                      point1, 
                                      point2
                                  )
                                  
                                  # ç¢ºä¿åœ¨æ­£ç¢ºè¨­å‚™ä¸Š
                                  if sample_aux_loss.device != device:
                                      sample_aux_loss = sample_aux_loss.to(device)
                                  
                                  # ç«‹å³ç´¯ç©ä¸¦åˆ†é›¢
                                  total_aux_loss = total_aux_loss + sample_aux_loss.detach()
                                  samples_collected += 1
                                  
                                  print(f"[LCP Debug] Sample {samples_collected}: aux_loss = {sample_aux_loss.item():.6f}")
                                  
                                  # ç«‹å³åˆªé™¤æ¨£æœ¬æå¤±
                                  del sample_aux_loss
                              
                          except (ValueError, TypeError) as e:
                              print(f"[LCP Debug] Error processing sample: {e}")
                              continue
                      
                      # ğŸ”´ ç«‹å³æ¸…ç†è³‡æ–™åº«æŸ¥è©¢çµæœ
                      del point1_xs, point1_ys, point2_xs, point2_ys
                      
                      if min_length > 0:
                          break
                          
                  except Exception as e:
                      print(f"[LCP Debug] Error accessing data for image_id={image_id}, class_id={class_id}: {e}")
                      continue
          
          # ğŸ”´ éšæ®µ4: æ¸…ç†æ‰€æœ‰æ‰¹æ¬¡ç›¸é—œè³‡æ–™
          del images, class_images, loc_targets, cls_targets, class_ids, class_sizes, transforms, boxes, img_sizes
          force_cleanup()
          log_memory("æ‰¹æ¬¡è³‡æ–™æ¸…ç†å¾Œ")
          
          # ğŸ”´ éšæ®µ5: è¨ˆç®—æœ€çµ‚çµæœ
          if samples_collected > 0:
              average_aux_loss = total_aux_loss / samples_collected
              print(f"[LCP Debug] Total samples processed: {samples_collected}/{MAX_SAMPLES}")
              print(f"[LCP Debug] Average auxiliary loss: {average_aux_loss.item():.6f}")
              
              # ğŸ”´ ç¢ºä¿è¿”å›çš„å¼µé‡ä¸ä¿ç•™è¨ˆç®—åœ–
              result = average_aux_loss.clone().detach().requires_grad_(True)
              del average_aux_loss, total_aux_loss
              
              force_cleanup()
              log_memory("å‡½æ•¸çµæŸ")
              
              return result
          else:
              print(f"[LCP Debug] No valid samples processed, returning zero loss")
              del total_aux_loss
              force_cleanup()
              return torch.tensor(0.0, device=device, requires_grad=True)
              
      except Exception as e:
          print(f"[LCP Error] Failed to compute auxiliary loss: {e}")
          import traceback
          traceback.print_exc()
          
          # æ¸…ç†æ‰€æœ‰å¯èƒ½çš„è®Šæ•¸
          try:
              del total_aux_loss
          except:
              pass
          
          force_cleanup()
          return torch.tensor(0.0, device=device, requires_grad=True)
    
      finally:
          # ğŸ”´ æœ€çµ‚æ¸…ç†ï¼šç¢ºä¿æ‰€æœ‰è³‡æºéƒ½è¢«é‡‹æ”¾
          force_cleanup()
          log_memory("æœ€çµ‚æ¸…ç†å¾Œ")

    def extract_samples_from_batch_alternative(self, batch_data):
        """
        æ›¿ä»£çš„æå–æ–¹æ³•ï¼Œé©ç”¨æ–¼ä¸åŒçš„æ•¸æ“šæ ¼å¼
        """
        samples = []
        
        if batch_data is None:
            return samples
        
        try:
            # å˜—è©¦å¾ loc_targets ä¸­æå–é‚Šç•Œæ¡†è³‡è¨Š
            if len(batch_data) >= 3:
                batch_images, batch_class_images, batch_loc_targets = batch_data[:3]
                
                if hasattr(batch_loc_targets, 'shape'):
                    batch_size = batch_loc_targets.shape[0]
                    
                    for i in range(batch_size):
                        # å¾ loc_targets ä¸­æå–ç¬¬ä¸€å€‹æœ‰æ•ˆçš„é‚Šç•Œæ¡†
                        loc_target = batch_loc_targets[i]  # [num_anchors, 4]
                        
                        if loc_target.numel() > 0:
                            # æ‰¾åˆ°ç¬¬ä¸€å€‹éé›¶çš„é‚Šç•Œæ¡†
                            non_zero_mask = (loc_target != 0).any(dim=1)
                            if non_zero_mask.any():
                                first_valid_idx = non_zero_mask.nonzero()[0].item()
                                bbox = loc_target[first_valid_idx]  # [4]
                                
                                # è½‰æ›ç‚ºåº§æ¨™æ ¼å¼
                                x1, y1, x2, y2 = bbox.tolist()
                                point1 = (float(x1), float(y1))
                                point2 = (float(x2), float(y2))
                            else:
                                point1 = (0.0, 0.0)
                                point2 = (100.0, 100.0)
                        else:
                            point1 = (0.0, 0.0)
                            point2 = (100.0, 100.0)
                        
                        samples.append((i, 0, point1, point2))
                        
        except Exception as e:
            print(f"[WARNING] æ›¿ä»£æå–æ–¹æ³•å¤±æ•—: {e}")
            # æœ€å¾Œçš„é™ç´šæ–¹æ¡ˆ
            batch_size = 1
            samples.append((0, 0, (0.0, 0.0), (100.0, 100.0)))
        
        return samples