{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d51eb0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from os2d.modeling.model import build_os2d_from_config\n",
    "from os2d.config import cfg\n",
    "import  os2d.utils.visualization as visualizer\n",
    "from os2d.structures.feature_map import FeatureMapSize\n",
    "from os2d.utils import setup_logger, read_image, get_image_size_after_resize_preserving_aspect_ratio\n",
    "from os2d.data import dataloader\n",
    "from os2d.modeling.model import build_os2d_from_config\n",
    "\n",
    "from os2d.data.dataloader import build_eval_dataloaders_from_cfg, build_train_dataloader_from_config\n",
    "from os2d.engine.train import trainval_loop\n",
    "from os2d.utils import set_random_seed, get_trainable_parameters, mkdir, save_config, setup_logger, get_data_path\n",
    "from os2d.engine.optimization import create_optimizer\n",
    "from os2d.config import cfg\n",
    "from os2d.utils.visualization import *\n",
    "import random\n",
    "import os2d.utils.visualization as visualizer\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "from os2d.utils import get_image_size_after_resize_preserving_aspect_ratio\n",
    "from src.util.detection import generate_detection_boxes\n",
    "from src.util.visualize import visualize_boxes_on_image\n",
    "from src.util.filter import DataLoaderDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fab2480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.is_cuda:\n",
    "    assert torch.cuda.is_available(), \"Do not have available GPU, but cfg.is_cuda == 1\"\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# random seed\n",
    "set_random_seed(cfg.random_seed, cfg.is_cuda)\n",
    "\n",
    "# Model\n",
    "net, box_coder, criterion, img_normalization, optimizer_state = build_os2d_from_config(cfg)\n",
    "\n",
    "# Optimizer\n",
    "parameters = get_trainable_parameters(net)\n",
    "optimizer = create_optimizer(parameters, cfg.train.optim, optimizer_state)\n",
    "\n",
    "# load the dataset\n",
    "data_path = get_data_path()\n",
    "dataloader_train, datasets_train_for_eval = build_train_dataloader_from_config(cfg, box_coder, img_normalization,\n",
    "                                                                                data_path=data_path)\n",
    "\n",
    "dataloaders_eval = build_eval_dataloaders_from_cfg(cfg, box_coder, img_normalization,\n",
    "                                                    datasets_for_eval=datasets_train_for_eval,\n",
    "                                                    data_path=data_path)\n",
    "\n",
    "db = DataLoaderDB( path = './src/db/data.csv' , dataloader = dataloader_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3688df13",
   "metadata": {},
   "source": [
    "### Test Basic Method of DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe117baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "image_ids = list ( map( int , db.get_image_ids()) )\n",
    "sorted_image_ids = sorted(image_ids)\n",
    "print( sorted_image_ids )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "604156a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 3, '1': 3, '2': 2, '3': 4, '4': 4, '5': 3, '6': 3, '7': 2, '1055': 6}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.get_class_ids_by_image_id(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b0f2f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(db.get_value_by_id( 10 , 42 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ab8e5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lcp.ct_aoi_align import ContextAoiAlign\n",
    "transform_image = transforms.Compose([\n",
    "                      transforms.ToTensor(),\n",
    "                      transforms.Normalize(img_normalization[\"mean\"], img_normalization[\"std\"])\n",
    "                      ])\n",
    "\n",
    "context_aoi_align = ContextAoiAlign( db, dataloader_train, transform_image , net , cfg )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b141b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_aoi_align.compute_roi_region_for_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6298099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context_aoi_align.extract_roi_features_for_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9524c3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lcp.aux_net import AuxiliaryNetwork\n",
    "aux_net = AuxiliaryNetwork( context_aoi_align, db )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ea74841",
   "metadata": {},
   "outputs": [],
   "source": [
    "point1_x = float( db.get_specific_data(0 , 0 , 'point1_x' )[0] )\n",
    "point1_y = float( db.get_specific_data(0 , 0 , 'point1_y' )[0] )\n",
    "point2_x = float( db.get_specific_data(0 , 0 , 'point2_x' )[0] )\n",
    "point2_y = float( db.get_specific_data(0 , 0 , 'point2_y' )[0] )   \n",
    "\n",
    "point1 = ( point1_x , point1_y )\n",
    "point2 = ( point2_x , point2_y )\n",
    "m = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8117b4c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8325454179212606, 0.8253786561656428, 0.9182732106013853]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux_net.giou(0 , 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b78be5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3652, device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux_net.ac_loss( 0 , 0 , point1 , point2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eabf8d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.1675)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux_net.ar_loss( 0 , 0 , point1 , point2 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48fcf6a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.5326, device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux_net.aux_loss( 0 , 0 , point1 , point2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9702a18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.JpegImagePlugin.JpegImageFile"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataloader_train._get_dataset_image_by_id(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fef114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lcp.lcp import LCP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8f51a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcp = LCP(net, aux_net, dataloader_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1c3ac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = lcp.get_image_tensor_from_dataloader(image_id=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ad699de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HOOK] net_feature_maps output mean: 0.39188438653945923, std: 0.9529884457588196, shape: torch.Size([1, 1024, 82, 109])\n",
      "Feature map shape: torch.Size([1, 1024, 82, 109])\n"
     ]
    }
   ],
   "source": [
    "# 保證 img_tensor 和模型在同一裝置\n",
    "if next(lcp._net.parameters()).is_cuda:\n",
    "    img_tensor = img_tensor.cuda()\n",
    "feature_map = lcp.get_layer_feature(img_tensor, layer_name='net_feature_maps')\n",
    "if feature_map is not None:\n",
    "    print(f\"Feature map shape: {feature_map.shape}\")\n",
    "else:\n",
    "    print(\"No feature map captured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3657d1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1: 64 channels\n",
      "layer1.0.conv1: 64 channels\n",
      "layer1.0.conv2: 64 channels\n",
      "layer1.0.conv3: 256 channels\n",
      "layer1.0.downsample.0: 256 channels\n",
      "layer1.1.conv1: 64 channels\n",
      "layer1.1.conv2: 64 channels\n",
      "layer1.1.conv3: 256 channels\n",
      "layer1.2.conv1: 64 channels\n",
      "layer1.2.conv2: 64 channels\n",
      "layer1.2.conv3: 256 channels\n",
      "layer2.0.conv1: 128 channels\n",
      "layer2.0.conv2: 128 channels\n",
      "layer2.0.conv3: 512 channels\n",
      "layer2.0.downsample.0: 512 channels\n",
      "layer2.1.conv1: 128 channels\n",
      "layer2.1.conv2: 128 channels\n",
      "layer2.1.conv3: 512 channels\n",
      "layer2.2.conv1: 128 channels\n",
      "layer2.2.conv2: 128 channels\n",
      "layer2.2.conv3: 512 channels\n",
      "layer2.3.conv1: 128 channels\n",
      "layer2.3.conv2: 128 channels\n",
      "layer2.3.conv3: 512 channels\n",
      "layer3.0.conv1: 256 channels\n",
      "layer3.0.conv2: 256 channels\n",
      "layer3.0.conv3: 1024 channels\n",
      "layer3.0.downsample.0: 1024 channels\n",
      "layer3.1.conv1: 256 channels\n",
      "layer3.1.conv2: 256 channels\n",
      "layer3.1.conv3: 1024 channels\n",
      "layer3.2.conv1: 256 channels\n",
      "layer3.2.conv2: 256 channels\n",
      "layer3.2.conv3: 1024 channels\n",
      "layer3.3.conv1: 256 channels\n",
      "layer3.3.conv2: 256 channels\n",
      "layer3.3.conv3: 1024 channels\n",
      "layer3.4.conv1: 256 channels\n",
      "layer3.4.conv2: 256 channels\n",
      "layer3.4.conv3: 1024 channels\n",
      "layer3.5.conv1: 256 channels\n",
      "layer3.5.conv2: 256 channels\n",
      "layer3.5.conv3: 1024 channels\n"
     ]
    }
   ],
   "source": [
    "layers = lcp.get_layers_name()\n",
    "for name, ch in layers:\n",
    "    print(f\"{name}: {ch} channels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "698be347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 取得所有 image_id\n",
    "# image_ids = list(map(int, db.get_image_ids()))\n",
    "# # 取得所有 layer name\n",
    "# layer_names = [name for name, ch in lcp.get_layers_name()]\n",
    "\n",
    "# for image_id in image_ids:\n",
    "#     img_tensor = lcp.get_image_tensor_from_dataloader(image_id=image_id)\n",
    "#     if next(lcp._net.parameters()).is_cuda:\n",
    "#         img_tensor = img_tensor.cuda()\n",
    "#     print(f\"\\n[Image ID: {image_id}]\")\n",
    "#     for layer_name in layer_names:\n",
    "#         try:\n",
    "#             feature_map = lcp.get_layer_feature(img_tensor, layer_name=f\"net_feature_maps.{layer_name}\")\n",
    "#             if feature_map is not None:\n",
    "#                 print(f\"{layer_name}: {tuple(feature_map.shape)}\")\n",
    "#             else:\n",
    "#                 print(f\"{layer_name}: No feature map captured.\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"{layer_name}: ERROR - {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f511fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 取得所有 layer name\n",
    "# layer_names = [name for name, ch in lcp.get_layers_name()]\n",
    "# for layer_name in layer_names:\n",
    "#     try:\n",
    "#         joint_loss, rec_loss_mean, aux_loss_mean = lcp.compute_joint_loss( layer_name=f\"net_feature_maps.{layer_name}\" , use_image_num=1)\n",
    "#         print(f\"{layer_name}: Joint Loss = {joint_loss}, Rec Loss = {rec_loss_mean}, Aux Loss = {aux_loss_mean}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"{layer_name}: ERROR - {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec38c724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 測試前，net_feature_maps.layer2.1.conv2 的重建誤差：\n",
      "[HOOK] net_feature_maps.layer2.1.conv2 output mean: -0.15752539038658142, std: 0.6267192959785461, shape: torch.Size([1, 128, 163, 217])\n",
      "[HOOK] net_feature_maps.layer2.1.conv2 output mean: -0.15752539038658142, std: 0.6267192959785461, shape: torch.Size([1, 128, 163, 217])\n",
      "[LOG] Image ID: 19, Layer: net_feature_maps.layer2.1.conv2, Loss: 0.0, Q: 4527488, torch.norm(feature_map_orig - feature_map_pruned, p=2) = 0.0\n",
      "[HOOK] net_feature_maps.layer2.1.conv2 output mean: -0.16169004142284393, std: 0.6381144523620605, shape: torch.Size([1, 128, 163, 217])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 70.00 MiB (GPU 0; 4.00 GiB total capacity; 10.43 GiB already allocated; 0 bytes free; 10.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_for_prune_channel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marti\\os2d-in-Channel-Prune\\src\\lcp\\lcp.py:307\u001b[0m, in \u001b[0;36mLCP.test_for_prune_channel\u001b[1;34m(self, num_images, num_modify, random_seed)\u001b[0m\n\u001b[0;32m    304\u001b[0m keep_out_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(out_channels \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, out_channels)  \u001b[38;5;66;03m# 只保留後半\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m==> 測試前，\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 的重建誤差：\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 307\u001b[0m rec_loss_before \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_reconstruction_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Before Prune] Reconstruction loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrec_loss_before\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    310\u001b[0m \u001b[38;5;66;03m# 1. Conv2 output channel slicing（物理剪掉一半channel）\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marti\\os2d-in-Channel-Prune\\src\\lcp\\lcp.py:152\u001b[0m, in \u001b[0;36mLCP.compute_reconstruction_loss\u001b[1;34m(self, image_ids, layer_name, keep_out_idx)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_id \u001b[38;5;129;01min\u001b[39;00m image_ids:\n\u001b[0;32m    151\u001b[0m     img_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_image_tensor_from_dataloader(image_id, is_cuda\u001b[38;5;241m=\u001b[39mis_cuda)\n\u001b[1;32m--> 152\u001b[0m     feature_map_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_layer_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m     feature_map_pruned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_prune_layer_feature(img_tensor, layer_name\u001b[38;5;241m=\u001b[39mlayer_name)\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m keep_out_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\marti\\os2d-in-Channel-Prune\\src\\lcp\\lcp.py:53\u001b[0m, in \u001b[0;36mLCP.get_layer_feature\u001b[1;34m(self, image_tensor, layer_name)\u001b[0m\n\u001b[0;32m     51\u001b[0m handle \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mregister_forward_hook(hook)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_net\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m---> 53\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet_feature_maps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m handle\u001b[38;5;241m.\u001b[39mremove()\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_features\u001b[38;5;241m.\u001b[39mget(layer_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\marti\\anaconda3\\envs\\ntut-project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\marti\\os2d-in-Channel-Prune\\os2d\\modeling\\feature_extractor.py:64\u001b[0m, in \u001b[0;36mResNetFeatureExtractor.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     61\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresnet_blocks:\n\u001b[1;32m---> 64\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\marti\\anaconda3\\envs\\ntut-project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\marti\\anaconda3\\envs\\ntut-project\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\marti\\anaconda3\\envs\\ntut-project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\marti\\anaconda3\\envs\\ntut-project\\lib\\site-packages\\torchvision\\models\\resnet.py:154\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    151\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[0;32m    152\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[1;32m--> 154\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(out)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\marti\\anaconda3\\envs\\ntut-project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\marti\\anaconda3\\envs\\ntut-project\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marti\\anaconda3\\envs\\ntut-project\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 0; 4.00 GiB total capacity; 10.43 GiB already allocated; 0 bytes free; 10.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "lcp.test_for_prune_channel(num_images=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3863c9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcp.compute_channel_importance( \n",
    "    layer_name='net_feature_maps.layer2.2.conv2', \n",
    "    use_image_num=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6b690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = [name for name, ch in lcp.get_layers_name()]\n",
    "for layer_name in layer_names:\n",
    "    try:\n",
    "        test = lcp.compute_channel_importance( \n",
    "            layer_name='net_feature_maps.layer2.2.conv2', \n",
    "            use_image_num=1\n",
    "        )\n",
    "        print(f\"{layer_name}: Computed channel importance successfully = {test}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"{layer_name}: ERROR - {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntut-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
